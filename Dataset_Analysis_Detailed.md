# Dataset Overview:

Dataset,Size,Speakers,Audio Quality,Best For...
LJ Speech,24 hrs,"1 (Female, US)",Good (22kHz),"Beginners, Single Voice, Benchmarking"
VCTK,44 hrs,109 (Global Accents),High (48kHz),"Accents, Voice Conversion research"
LibriTTS,585 hrs,"2,400+ (Various)",Medium (24kHz),"Voice Cloning, Generalization, ""Big AI"" models"
Hi-Fi TTS,292 hrs,10 (Studio Pros),Excellent (44.1kHz),"Commercial Quality, High-Fidelity Audio"
HUI (German),326 hrs,5 Major + 117 Minor,Excellent (44.1kHz),German Language SOTA

# Voice Quality Mapping

In Machine Learning, this relationship is absolute: The model creates a mathematical probability distribution of the data. It does not "know" what a voice is; it only knows what your data looks like.

## 1. Clarity

The "Definition" and "Sharpness" of the audio.

### Signal-to-Noise Ratio (SNR) → The Noise Floor

Mechanism: Neural networks are "generative." They don't just generate the voice; they generate the silence too. If your dataset has a constant background hum (like a computer fan), the model learns that "silence = hum."

Result: A model trained on noisy audio will produce a voice that sounds like it is permanently speaking through a bad phone line. You cannot filter this out easily after training because the noise is intertwined with the speech frequencies.

### Sample Rate → The Frequency Ceiling

Mechanism: The sample rate defines the highest pitch frequency the model can reproduce (The Nyquist Limit).

16kHz (8kHz limit): Sounds "muffled," like AM radio. Fricatives (sounds like "s", "f", "th") lose their crispness because they live in high frequencies.

22.05kHz (11kHz limit): The standard for years (e.g., LJSpeech). Good, but lacks "air" or "sparkle."

44.1kHz+ (22kHz limit): "Hi-Fi." Captures the breathiness and subtle mouth clicks that make audio sound "real."

### Articulation Quality → Alignment Stability

Mechanism: If the speaker in the dataset mumbles or slurs words (e.g., saying "gonna" instead of "going to" when the text says "going to"), the Attention Mechanism (the part of the AI that matches text to sound) gets confused.

Result: The model produces "glitches"—skipping words, repeating syllables, or slurring speech.

## 2. Naturalness

The "Human-ness" and flow.

### Prosody (Intonation) → The "Melody" of Speech

Mechanism: The model learns the statistical variance in pitch.

The Problem: Many datasets use "Audiobook Read Style," which is very steady and rhythmic.

The Result: The model sounds pleasant but hypnotic and robotic over long periods because it repeats the same "falling pitch at the end of every sentence" pattern. It lacks the spontaneous pitch spikes of real conversation.

### Rhythm & Pauses → Breath Modeling

Mechanism: In real speech, we pause to think or breathe. If the training data has all breaths aggressively cut out (silence trimming), the model learns that "humans never stop talking."

Result: The synthesized voice will sound rushed, like a person running a marathon while reading. To fix this, modern datasets leave the breath sounds in so the model learns when to breathe.

## 3. Accent

The regional "flavor" of pronunciation.

Phoneme Mapping → The "Vowel Shift"

Mechanism: The Piper JSON file maps a symbol (like a) to a sound. The model learns the specific sound wave associated with that symbol from that specific speaker.

Example: If you train on an Australian speaker, the model learns that the phoneme [eɪ] (as in "day") sounds like [aɪ] (closer to "die").

Transfer: When you later type "Have a good day," the model retrieves the Australian version of those vowels, effectively "transferring" the accent perfectly.

Consistency is King: If your dataset mixes a US speaker and a UK speaker, the model calculates the "average" of their vowels. The result is a "Ghost Accent"—a weird, drifting voice that sounds like neither.

## 4. Pitch & Timbre

The Identity of the speaker.

### Fundamental Frequency (F0) → Pitch

Mechanism: The model mimics the vibration rate of the speaker's vocal cords.

Result: You cannot simply "turn a knob" to make a trained female voice sound like a male voice. If you lower the pitch of a female model, it sounds like a female voice slowed down (sluggish), not like a biological male voice, because the timbre (resonance) remains female.

### Timbre (Spectral Envelope) → The Body

Mechanism: Timbre is determined by the shape of the throat, nose, and mouth. This is represented in the dataset as the "Spectral Envelope."

Result: If your speaker has a nasal voice, the model learns the mathematical shape of a nasal cavity. This is why "Voice Cloning" works—it copies the shape of the vocal tract from the audio.

## 5. Emotional Conveyance

The ability to feel.

Range in Training Data → The "Latent Space"

Mechanism: A model cannot output what it has never seen.

The Problem: 99% of datasets (like LJSpeech) are "Neutral/Calm."

The Limitation: If you ask an LJSpeech model to "Scream in terror," it fails. It physically cannot generate a scream because it has no mathematical data on what a scream looks like in a spectrogram.

Expressive Samples: To get emotion, the dataset must include "Style Tokens." You need 1 hour of the speaker whispering, 1 hour of them yelling, and 1 hour of them laughing. The model then learns a "slider" between these states.

# Dataset Selection Guidelines

The following table compares available datasets based on speaker attributes, style, and emotional range to help select the best training source for your target profile.

| Dataset              | Gender & Age                | Accent                              | Emotion Range                           | Speaking Style                         | Best For (Target Profile)                                                                                      |
| :------------------- | :-------------------------- | :---------------------------------- | :-------------------------------------- | :------------------------------------- | :------------------------------------------------------------------------------------------------------------- |
| **LJ Speech**        | Female Adult (Young)        | US (North American Neutral)         | Low (Consistent, Neutral)               | Read / Formal (Non-fiction Audiobook)  | **AI Assistants**<br>Ideal for standard, pleasant, and clear "Siri-like" voices.                               |
| **LibriTTS** (Clean) | Mixed (M & F, Various Ages) | Mostly US (Some UK/Other)           | Medium (Storytelling dynamic)           | Read / Narrative (Fiction Audiobooks)  | **Narrators**<br>Best for reading long articles or books where natural flow matters more than perfect clarity. |
| **VCTK**             | Mixed (109 speakers)        | Global (Scottish, Indian, US, etc.) | Low (Newspaper reading)                 | Read / Neutral (Short sentences)       | **Global/Accent Work**<br>Use when a specific regional accent or voice conversion capability is required.      |
| **ESD** (Emotional)  | Mixed (5 Male, 5 Female)    | Mandarin & English                  | High / Tagged (Happy, Sad, Angry, etc.) | Acted (Emotional performance)          | **Gaming / Characters**<br>When the AI needs to "perform" or react emotionally to user input.                  |
| **Expresso** (New)   | Mixed (2 Male, 2 Female)    | US                                  | High / Stylized (Whisper, Projecting)   | Conversational (Filled pauses, laughs) | **Chatbots**<br>The most human-sounding data available for casual, conversational interactions.                |
| **Hi-Fi TTS**        | Mixed (6 Male, 4 Female)    | US / EU                             | Medium (Professional)                   | Read / Studio (High Fidelity)          | **Commercial / Ads**<br>Use when audio quality and crispness are the highest priorities.                       |

# Data Preparation Pipeline

## 1. Preprocessing Steps (The "Raw Material" Prep)

Convert Audio: Convert all files to WAV format (16-bit PCM).

Resample: Downsample/Upsample to your target rate (Standard: 22050 Hz, Hi-Fi: 44100 Hz).

Mono Conversion: Merge stereo channels into mono (unless specifically training a spatial model).

Segmentation: Slice long audio files into short clips (1 to 10 seconds). Ideally, split at natural sentence pauses.

## 2. Quality Checks (The "Filter")

SNR Check: Discard files with high background noise (target >60dB SNR).

Length Filter:

Discard < 0.5s: Too short (often just a breath or click).

Discard > 10s: Too long (hard for the model to learn attention/alignment).

Spot Check: Manually listen to random samples. If you hear mouse clicks, breathing heavy on the mic, or echo—clean or discard.

## 3. Normalization Procedures (Standardizing)

Loudness Normalization: Normalize all clips to the same volume standard (e.g., -23 LUFS or peak normalization to -1.0 dB). This prevents the voice from getting randomly loud/quiet.

Silence Trimming: Trim leading/trailing silence to exactly 0.1s.

Too much silence = Model learns to hesitate.

Zero silence = Audio sounds chopped.

Text Normalization:

Expand Numbers: 1990 → "nineteen ninety".

Expand Abbreviations: Dr. → "Doctor", St. → "Street".

Expand Currency: $5 → "five dollars".

Remove Illegal Chars: Strip emojis or symbols not in your phoneme set.

## 4. Alignment Requirements (Linking Text to Audio)

MFA (Montreal Forced Aligner): Run this tool to generate a "TextGrid." It calculates exactly where each word starts and ends in the audio.

Phonemization: Convert your clean text into phonemes (e.g., "Hello" → HH AH0 L OW1) using a tool like eSpeak-ng.

Validation: Check if the duration of the text matches the duration of the audio.

Audio is 5s but text is 50 words? → Mismatch. Discard.

Audio is 10s but text is "Hi"? → Mismatch. Discard.

